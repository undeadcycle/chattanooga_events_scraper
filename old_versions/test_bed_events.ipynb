{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad83f579-cc41-4828-b3f6-45237ef717ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and parsing Times Free Press\n",
      "Found Shadow DOM host: div, id: evvnt-calendar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x75aab172ef90>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/8215fe73026b26e54962e4886793bc60/execute/sync\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x75aab172fed0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/8215fe73026b26e54962e4886793bc60/execute/sync\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x75aab1730d50>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/8215fe73026b26e54962e4886793bc60/execute/sync\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No events found in main content, checking Shadow DOM...\n"
     ]
    },
    {
     "ename": "MaxRetryError",
     "evalue": "HTTPConnectionPool(host='localhost', port=54819): Max retries exceeded with url: /session/8215fe73026b26e54962e4886793bc60/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x75aab1731bd0>: Failed to establish a new connection: [Errno 111] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    199\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[1;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    201\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[1;32m    202\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[1;32m    203\u001b[0m     )\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:793\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 793\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    794\u001b[0m     conn,\n\u001b[1;32m    795\u001b[0m     method,\n\u001b[1;32m    796\u001b[0m     url,\n\u001b[1;32m    797\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    798\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    799\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    800\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    801\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[1;32m    802\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[1;32m    803\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    804\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    806\u001b[0m )\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    497\u001b[0m         method,\n\u001b[1;32m    498\u001b[0m         url,\n\u001b[1;32m    499\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    500\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    501\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    502\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    503\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    504\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:400\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1289\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1048\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1051\u001b[0m \n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:986\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:238\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Audit hooks are only available in Python 3.8+\u001b[39;00m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x75aab1731bd0>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 322\u001b[0m\n\u001b[1;32m    319\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Be polite, wait a second between requests\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 306\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m events:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo events found in main content, checking Shadow DOM...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 306\u001b[0m     shadow_contents \u001b[38;5;241m=\u001b[39m extract_shadow_dom_content(driver)\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shadow_content \u001b[38;5;129;01min\u001b[39;00m shadow_contents:\n\u001b[1;32m    308\u001b[0m         shadow_parsed \u001b[38;5;241m=\u001b[39m parse_html(shadow_content)\n",
      "Cell \u001b[0;32mIn[1], line 192\u001b[0m, in \u001b[0;36mextract_shadow_dom_content\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_shadow_dom_content\u001b[39m(driver):\n\u001b[1;32m    176\u001b[0m     script \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124m    function getAllShadowContent(root) \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124m        var result = [];\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124m    return getAllShadowContent(document.body);\u001b[39m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver\u001b[38;5;241m.\u001b[39mexecute_script(script)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:414\u001b[0m, in \u001b[0;36mWebDriver.execute_script\u001b[0;34m(self, script, *args)\u001b[0m\n\u001b[1;32m    411\u001b[0m converted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args)\n\u001b[1;32m    412\u001b[0m command \u001b[38;5;241m=\u001b[39m Command\u001b[38;5;241m.\u001b[39mW3C_EXECUTE_SCRIPT\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(command, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscript\u001b[39m\u001b[38;5;124m\"\u001b[39m: script, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: converted_args})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:352\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    350\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 352\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    300\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    319\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/_request_methods.py:144\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    137\u001b[0m         method,\n\u001b[1;32m    138\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[1;32m    145\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m    146\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/_request_methods.py:279\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    275\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    277\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/poolmanager.py:444\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    442\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    446\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:877\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    874\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    876\u001b[0m     )\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    878\u001b[0m         method,\n\u001b[1;32m    879\u001b[0m         url,\n\u001b[1;32m    880\u001b[0m         body,\n\u001b[1;32m    881\u001b[0m         headers,\n\u001b[1;32m    882\u001b[0m         retries,\n\u001b[1;32m    883\u001b[0m         redirect,\n\u001b[1;32m    884\u001b[0m         assert_same_host,\n\u001b[1;32m    885\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    886\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    887\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    888\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    889\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    890\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    891\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    896\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:877\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    874\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    876\u001b[0m     )\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    878\u001b[0m         method,\n\u001b[1;32m    879\u001b[0m         url,\n\u001b[1;32m    880\u001b[0m         body,\n\u001b[1;32m    881\u001b[0m         headers,\n\u001b[1;32m    882\u001b[0m         retries,\n\u001b[1;32m    883\u001b[0m         redirect,\n\u001b[1;32m    884\u001b[0m         assert_same_host,\n\u001b[1;32m    885\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    886\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    887\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    888\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    889\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    890\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    891\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    896\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:877\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    874\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    876\u001b[0m     )\n\u001b[0;32m--> 877\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    878\u001b[0m         method,\n\u001b[1;32m    879\u001b[0m         url,\n\u001b[1;32m    880\u001b[0m         body,\n\u001b[1;32m    881\u001b[0m         headers,\n\u001b[1;32m    882\u001b[0m         retries,\n\u001b[1;32m    883\u001b[0m         redirect,\n\u001b[1;32m    884\u001b[0m         assert_same_host,\n\u001b[1;32m    885\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    886\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[1;32m    887\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[1;32m    888\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    889\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[1;32m    890\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[1;32m    891\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[1;32m    893\u001b[0m     )\n\u001b[1;32m    895\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    896\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:847\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_e, (\u001b[38;5;167;01mOSError\u001b[39;00m, HTTPException)):\n\u001b[1;32m    845\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 847\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    848\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    849\u001b[0m )\n\u001b[1;32m    850\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n\u001b[1;32m    852\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_retry\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=54819): Max retries exceeded with url: /session/8215fe73026b26e54962e4886793bc60/execute/sync (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x75aab1731bd0>: Failed to establish a new connection: [Errno 111] Connection refused'))"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import chromedriver_autoinstaller\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import time\n",
    "import re\n",
    "\n",
    "####################\n",
    "# CONFIGURATION\n",
    "####################\n",
    "\n",
    "SITES = {\n",
    "\n",
    "    # \"Visit Chattanooga\": {\n",
    "    #     \"url\": \"https://www.visitchattanooga.com/events/\",\n",
    "    #     \"content_list_class\": {\"div\": {\"class\": \"content grid\"}},\n",
    "    #     \"item_attr\": {\"div\": {\"data-type\": \"events\"}},\n",
    "    #     \"title\": {\"a\": {\"class\": \"title truncate\"}},\n",
    "    #     \"date\": {\"span\": {\"class\": \"mini-date-container\"}}, # now broken since dateutil. Check for start and end times\n",
    "    #     \"img\": { # Need to display as image\n",
    "    #         \"img\": {\"class\": \"thumb\"},\n",
    "    #         \"parse_method\": \"lazy-src\"\n",
    "    #     },\n",
    "    #     \"location\": {\"li\": {\"class\": \"locations truncate\"}},\n",
    "    #     \"recurrence\": {\"li\": {\"class\": \"recurrence\"}}\n",
    "    # },\n",
    "\n",
    "    # \"CHA Guide Events\": {\n",
    "    #     \"url\": \"https://www.cha.guide/events\",\n",
    "    #     \"content_list_class\": {\"div\": {\"class\": \"flex-table w-dyn-items\"}},\n",
    "    #     \"item_attr\": {\"div\": {\"role\": \"listitem\"}},\n",
    "    #     \"title\": {\"h3\": {\"class\": \"event-title\"}},\n",
    "    #     \"date\": {\"div\": {\"class\": \"event-date-div\"}}, # now broken since dateutil. Check for start and end times\n",
    "    #     \"img\": {\n",
    "    #         \"parse_method\": \"none\",\n",
    "    #     },\n",
    "    #     \"location\": {\"div\": {\"class\": \"location-2\"}},\n",
    "    #     \"recurrence\": {\"div\": {\"class\": \"event---category-circle\"}}\n",
    "        # \"category\": {\"div\": {\"class\": \"event---category-circle\"}}\n",
    "        # \"details\": {\"div\": {\"class\": \"smaller-text bottom-margin---5px\"}}\n",
    "    # },\n",
    "    \n",
    "    # \"Chattanooga Pulse\": {\n",
    "    #     \"url\": \"https://www.chattanoogapulse.com/search/event/the-pulse-event-search/#page=1\", # no event url\n",
    "    #     \"content_list_class\": {\"div\": {\"id\": \"event_list_div\"}},\n",
    "    #     \"item_attr\": {\"div\": {\"class\": \"event_result\"}},\n",
    "    #     \"title\": {\"h4\": {\"class\": \"event_title\"}},\n",
    "    #     \"date\": {\"p\": {\"class\": \"event_date\"}}, # need to format as AM/PM\n",
    "    #     \"img\": { # Need to display as image\n",
    "    #         \"container\": {\"div\": {\"class\": \"event_thumb\"}},\n",
    "    #         \"tag\": \"img\",\n",
    "    #         \"attr\": \"srcset\",\n",
    "    #         \"parse_method\": \"srcset_220w\"\n",
    "    #     },\n",
    "    #     \"location\": {\"a\": {}}, # outputs title instead of location\n",
    "    #     \"recurrence\": {\"p\": {\"class\": \"cats\"}}\n",
    "        # Add a description or details section\n",
    "\n",
    "    # },\n",
    "    \n",
    "    # \"Nooga Today\": {\n",
    "    #     \"url\": \"https://noogatoday.6amcity.com/events#/\", # preloaded_lightbox blocking site\n",
    "    #     },\n",
    "    \n",
    "    # \"Choose Chatt\": {\n",
    "    #     \"url\": \"https://choosechatt.com/chattanooga-events/\", # dialog-lightbox-message blocking site\n",
    "    # },\n",
    "\n",
    "    \"Times Free Press\": {\n",
    "        \"url\": \"https://www.timesfreepress.com/tfpevents/?_evDiscoveryPath=/\",\n",
    "        \"content_list_class\": {\"div\": {\"class\": \"ev-events-container\"}},\n",
    "        \"item_attr\": {\"div\": {\"class\": \"ev-event-card\"}},\n",
    "        \"title\": {\"h3\": {\"class\": \"ev-card-title\"}},\n",
    "        \"date\": {\"div\": {\"class\": \"ev-card-start\"}},\n",
    "        \"img\": {\n",
    "            \"img\": {\"class\": \"ev-card-image\"},\n",
    "            \"parse_method\": \"src\"\n",
    "        },\n",
    "        \"location\": {\"div\": {\"class\": \"ev-card-venue\"}},\n",
    "        \"price\": {\"div\": {\"class\": \"ev-card-price\"}}\n",
    "    },\n",
    "\n",
    "    # \"CHA Guide Weekly\": {\n",
    "    #     \"url\": \"https://www.cha.guide/explore/things-to-do-in-chattanooga-this-week\",\n",
    "    # },\n",
    "\n",
    "    # \"Chattanooga Chamber\": {\n",
    "    #     \"url\": \"https://chattanoogachamber.com/\",\n",
    "    # },\n",
    "\n",
    "    # \"Chattanooga Library\": {\n",
    "    #     \"url\": \"https://www.chattlibrary.org/events/\",\n",
    "    # }\n",
    "\n",
    "}\n",
    "\n",
    "####################\n",
    "# FETCHING & PARSING\n",
    "####################\n",
    "\n",
    "def fetch_page(url):\n",
    "    # options = Options()\n",
    "    # options.add_argument(\"--headless\")  # Run in headless mode (no GUI)\n",
    "    # service = Service(ChromeDriverManager().install())\n",
    "    chromedriver_autoinstaller.install()\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for JavaScript to load content\n",
    "        check_shadow_dom(driver) #DEBUGGING\n",
    "        scroll_page(driver) #DEBUGGING\n",
    "        return driver, driver.page_source\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching the page: {e}\")\n",
    "        return None, None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def parse_html(html_content):\n",
    "    return BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "####################\n",
    "# DEBUGGING \n",
    "####################\n",
    "\n",
    "def grid_search(html_content):\n",
    "    grid_match = re.search(r'<div[^>]*class=\"[^\"]*grid[^\"]*\"', html_content)\n",
    "    if grid_match:\n",
    "        start = max(0, grid_match.start() - 500)\n",
    "        end = min(len(html_content), grid_match.end() + 1000)\n",
    "        print(html_content[start:end])\n",
    "    else:\n",
    "        print(\"Could not find 'grid' class in the HTML\")\n",
    "\n",
    "def find_iframes(html_content):\n",
    "    iframes = parsed_content.find_all('iframe')\n",
    "    for iframe in iframes:\n",
    "        print(f\"Found iframe: {iframe.get('src', 'No src attribute')}\")\n",
    "\n",
    "def check_shadow_dom(driver):\n",
    "    shadow_hosts = driver.execute_script(\"\"\"\n",
    "        return Array.from(document.querySelectorAll('*')).filter(el => el.shadowRoot);\n",
    "    \"\"\")\n",
    "    for host in shadow_hosts:\n",
    "        print(f\"Found Shadow DOM host: {host.tag_name}, id: {host.get_attribute('id')}\")\n",
    "\n",
    "def find_potential_containers(parsed_content):\n",
    "    potential_containers = parsed_content.find_all('div', class_=lambda x: x and any(keyword in x.lower() for keyword in ['list', 'container', 'wrapper', 'events']))\n",
    "    for container in potential_containers:\n",
    "        print(f\"Potential container found: {container.get('class')}\")\n",
    "\n",
    "def scroll_page(driver):\n",
    "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for i in range(1, total_height, 100):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {i});\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "def save_html(html_content):\n",
    "    with open('page_source.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "# EXTRACTION \n",
    "####################\n",
    "\n",
    "def extract_shadow_dom_content(driver):\n",
    "    script = \"\"\"\n",
    "    function getAllShadowContent(root) {\n",
    "        var result = [];\n",
    "        var walker = document.createTreeWalker(root, NodeFilter.SHOW_ELEMENT);\n",
    "        var node;\n",
    "\n",
    "        while (node = walker.nextNode()) {\n",
    "            if (node.shadowRoot) {\n",
    "                result.push(node.shadowRoot.innerHTML);\n",
    "                result = result.concat(getAllShadowContent(node.shadowRoot));\n",
    "            }\n",
    "        }\n",
    "        return result;\n",
    "    }\n",
    "    return getAllShadowContent(document.body);\n",
    "    \"\"\"\n",
    "    return driver.execute_script(script)\n",
    "\n",
    "def extract_image_url(img_config, item):\n",
    "    if img_config['parse_method'] == 'lazy-src':\n",
    "        img_tag, img_attrs = next(iter(img_config.items()))\n",
    "        img_element = item.find(img_tag, **img_attrs)\n",
    "        return img_element.get('data-lazy-src') or img_element.get('src') if img_element else None\n",
    "\n",
    "    elif img_config['parse_method'] == 'srcset_220w':\n",
    "        container_tag, container_attrs = next(iter(img_config['container'].items()))\n",
    "        container = item.find(container_tag, **container_attrs)\n",
    "        if not container:\n",
    "            return None\n",
    "\n",
    "        img_element = container.find(img_config['tag'])\n",
    "        if not img_element or img_config['attr'] not in img_element.attrs:\n",
    "            return None\n",
    "\n",
    "        srcset = img_element[img_config['attr']]\n",
    "        urls = srcset.split(', ')\n",
    "        for url in urls:\n",
    "            if '220w' in url:\n",
    "                return url.split(' ')[0]\n",
    "        return urls[0].split(' ')[0] if urls else None\n",
    "\n",
    "    elif img_config['parse_method'] == 'none':\n",
    "        return None\n",
    "    \n",
    "    # Add other parsing methods here if needed\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_date_range(date_text):\n",
    "    # Regular expression to match date, start time, and end time\n",
    "    pattern = r'(\\w+ \\d+, \\d{4})(?: (\\d+:\\d+ [AP]M))?(?: - (\\d+:\\d+ [AP]M))?'\n",
    "    match = re.match(pattern, date_text)\n",
    "    \n",
    "    if match:\n",
    "        date_str, start_time, end_time = match.groups()\n",
    "        \n",
    "        try:\n",
    "            # Parse the date\n",
    "            parsed_date = parser.parse(date_str)\n",
    "            date = parsed_date.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            # Parse start time if available\n",
    "            start = None\n",
    "            if start_time:\n",
    "                start = parser.parse(f\"{date_str} {start_time}\").strftime(\"%H:%M\")\n",
    "            \n",
    "            # Parse end time if available\n",
    "            end = None\n",
    "            if end_time:\n",
    "                end = parser.parse(f\"{date_str} {end_time}\").strftime(\"%H:%M\")\n",
    "            \n",
    "            return {\n",
    "                'date': date,\n",
    "                'start_time': start,\n",
    "                'end_time': end\n",
    "            }\n",
    "        except ValueError:\n",
    "            print(f\"Couldn't parse date: {date_text}\")\n",
    "    \n",
    "    return {\n",
    "        'date': None,\n",
    "        'start_time': None,\n",
    "        'end_time': None\n",
    "    }\n",
    "\n",
    "# TODO: category function\n",
    "\n",
    "# TODO: details function\n",
    "\n",
    "def extract_events(parsed_content, config):\n",
    "    events = []\n",
    "    content_list = parsed_content.find(**config['content_list_class'])\n",
    "    \n",
    "    if not content_list:\n",
    "        return events\n",
    "\n",
    "    items = content_list.find_all(**config['item_attr'])\n",
    "    \n",
    "    for item in items:\n",
    "        event = {}\n",
    "        \n",
    "        for field, selector in config.items():\n",
    "            if field not in ['url', 'content_list_class', 'item_attr']:\n",
    "                element = item.find(**selector) if selector else None\n",
    "                if element:\n",
    "                    if field == 'img':\n",
    "                        event['image_url'] = extract_image_url(selector, element)\n",
    "                    else:\n",
    "                        event[field] = element.text.strip()\n",
    "        \n",
    "        if event:\n",
    "            events.append(event)\n",
    "    \n",
    "    return events\n",
    "\n",
    "####################\n",
    "# EXECUTION\n",
    "####################\n",
    "\n",
    "def main():\n",
    "    for site_name, config in SITES.items():\n",
    "        url = config[\"url\"]\n",
    "        print(f\"Fetching and parsing {site_name}\")\n",
    "        driver, html_content = fetch_page(url)\n",
    "        if driver and html_content:\n",
    "            parsed_content = parse_html(html_content)\n",
    "            events = extract_events(parsed_content, config)\n",
    "            \n",
    "            if not events:\n",
    "                print(\"No events found in main content, checking Shadow DOM...\")\n",
    "                shadow_contents = extract_shadow_dom_content(driver)\n",
    "                for shadow_content in shadow_contents:\n",
    "                    shadow_parsed = parse_html(shadow_content)\n",
    "                    shadow_events = extract_events(shadow_parsed, config)\n",
    "                    events.extend(shadow_events)\n",
    "            \n",
    "            print(f\"Extracted {len(events)} events\")\n",
    "            for event in events:\n",
    "                print(event)\n",
    "            \n",
    "            driver.quit()\n",
    "        else:\n",
    "            print(f\"Skipping {site_name} due to fetch error\")\n",
    "        time.sleep(1)  # Be polite, wait a second between requests\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "####################\n",
    "# BUGS & TODOS\n",
    "####################\n",
    "\n",
    "\n",
    "# TODO: category function\n",
    "\n",
    "# TODO: details function\n",
    "\n",
    "# BUG: rewrite date function to work on all sites (output times as AM/PM)\n",
    "\n",
    "# TODO: add a time and price function (time could be included in date function)\n",
    "\n",
    "# TODO: pull individual functions out of extract_events\n",
    "\n",
    "# TODO: display image link as an image instead of a url\n",
    "\n",
    "# BUG: fix event url and location in pulse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15c8988-7c56-46a1-abd2-711dd2d57d42",
   "metadata": {},
   "source": [
    "# GPT4o refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10584d81-6f2c-4d4b-9b1d-6d87f9ae8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import chromedriver_autoinstaller\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "\n",
    "\n",
    "####################\n",
    "# CONFIGURATION\n",
    "####################\n",
    "\n",
    "# Setup logging to a file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='web_scraper.log',  # Specify the log file name\n",
    "    filemode='w'  # 'w' to overwrite the log file each time, 'a' to append to it\n",
    ")\n",
    "\n",
    "SITES = {\n",
    "    \"Visit Chattanooga\": {\n",
    "        \"url\": \"https://www.visitchattanooga.com/events/\",\n",
    "        \"content_list_class\": {\"div\": {\"class\": \"content grid\"}},\n",
    "        \"item_attr\": {\"div\": {\"data-type\": \"events\"}},\n",
    "        \"title\": {\"a\": {\"class\": \"title truncate\"}},\n",
    "        \"date\": {\"span\": {\"class\": \"mini-date-container\"}},  # now broken since dateutil. Check for start and end times\n",
    "        \"img\": {  # Need to display as image\n",
    "            \"img\": {\"class\": \"thumb\"},\n",
    "            \"parse_method\": \"lazy-src\"\n",
    "        },\n",
    "        \"location\": {\"li\": {\"class\": \"locations truncate\"}},\n",
    "        \"recurrence\": {\"li\": {\"class\": \"recurrence\"}},\n",
    "    },\n",
    "    # Other sites...\n",
    "}\n",
    "\n",
    "####################\n",
    "# FETCHING & PARSING\n",
    "####################\n",
    "\n",
    "def fetch_page(url):\n",
    "    chromedriver_autoinstaller.install()\n",
    "    driver = webdriver.Chrome()\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for JavaScript to load content\n",
    "        scroll_page(driver)  # Scroll the page to ensure all content is loaded\n",
    "        return driver.page_source\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching the page: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def parse_html(html_content):\n",
    "    return BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "####################\n",
    "# DEBUGGING \n",
    "####################\n",
    "\n",
    "def grid_search(html_content):\n",
    "    grid_match = re.search(r'<div[^>]*class=\"[^\"]*grid[^\"]*\"', html_content)\n",
    "    if grid_match:\n",
    "        start = max(0, grid_match.start() - 500)\n",
    "        end = min(len(html_content), grid_match.end() + 1000)\n",
    "        logging.info(f\"Grid search result: {html_content[start:end]}\")\n",
    "    else:\n",
    "        logging.info(\"Could not find 'grid' class in the HTML\")\n",
    "\n",
    "def find_iframes(html_content):\n",
    "    iframes = html_content.find_all('iframe')\n",
    "    for iframe in iframes:\n",
    "        logging.info(f\"Found iframe: {iframe.get('src', 'No src attribute')}\")\n",
    "\n",
    "def check_shadow_dom(driver):\n",
    "    shadow_hosts = driver.execute_script(\"\"\"\n",
    "        return Array.from(document.querySelectorAll('*')).filter(el => el.shadowRoot);\n",
    "    \"\"\")\n",
    "    for host in shadow_hosts:\n",
    "        logging.info(f\"Found Shadow DOM host: {host.tag_name}, id: {host.get_attribute('id')}\")\n",
    "\n",
    "def find_potential_containers(parsed_content):\n",
    "    potential_containers = parsed_content.find_all('div', class_=lambda x: x and any(keyword in x.lower() for keyword in ['list', 'container', 'wrapper', 'events']))\n",
    "    for container in potential_containers:\n",
    "        logging.info(f\"Potential container found: {container.get('class')}\")\n",
    "\n",
    "def scroll_page(driver):\n",
    "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for i in range(1, total_height, 100):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {i});\")\n",
    "        time.sleep(0.1)\n",
    "\n",
    "def save_html(html_content, site_name):\n",
    "    with open(f'{site_name}.html', 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "\n",
    "####################\n",
    "# EXTRACTION \n",
    "####################\n",
    "\n",
    "def extract_image_url(img_config, item):\n",
    "    if img_config['parse_method'] == 'lazy-src':\n",
    "        img_tag, img_attrs = next(iter(img_config.items()))\n",
    "        img_element = item.find(img_tag, **img_attrs)\n",
    "        return img_element.get('data-lazy-src') or img_element.get('src') if img_element else None\n",
    "\n",
    "    elif img_config['parse_method'] == 'srcset_220w':\n",
    "        container_tag, container_attrs = next(iter(img_config['container'].items()))\n",
    "        container = item.find(container_tag, **container_attrs)\n",
    "        if not container:\n",
    "            return None\n",
    "\n",
    "        img_element = container.find(img_config['tag'])\n",
    "        if not img_element or img_config['attr'] not in img_element.attrs:\n",
    "            return None\n",
    "\n",
    "        srcset = img_element[img_config['attr']]\n",
    "        urls = srcset.split(', ')\n",
    "        for url in urls:\n",
    "            if '220w' in url:\n",
    "                return url.split(' ')[0]\n",
    "        return urls[0].split(' ')[0] if urls else None\n",
    "\n",
    "    elif img_config['parse_method'] == 'none':\n",
    "        return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "def parse_date_range(date_text):\n",
    "    try:\n",
    "        date_info = parser.parse(date_text, fuzzy=True)\n",
    "        date = date_info.strftime(\"%Y-%m-%d\")\n",
    "        return {\n",
    "            'date': date,\n",
    "            'start_time': date_info.strftime(\"%H:%M\") if date_info.hour else None,\n",
    "            'end_time': None\n",
    "        }\n",
    "    except ValueError:\n",
    "        logging.error(f\"Couldn't parse date: {date_text}\")\n",
    "        return {\n",
    "            'date': None,\n",
    "            'start_time': None,\n",
    "            'end_time': None\n",
    "        }\n",
    "\n",
    "def extract_events(parsed_content, config):\n",
    "    events = []\n",
    "    logging.info(f\"Searching for content list with: {config['content_list_class']}\")\n",
    "    \n",
    "    tag, class_name = next(iter(config['content_list_class'].items()))\n",
    "    content_list = None\n",
    "    \n",
    "    if 'id' in class_name:\n",
    "        content_list = parsed_content.find(tag, id=class_name['id'])\n",
    "        logging.info(f\"Attempting to find content list by id: {class_name['id']}\")\n",
    "\n",
    "    if not content_list and 'class' in class_name:\n",
    "        content_list = parsed_content.find(tag, class_=class_name['class'])\n",
    "        logging.info(f\"Attempting to find content list by class: {class_name['class']}\")\n",
    "\n",
    "    if not content_list:\n",
    "        content_list = parsed_content.find(tag)\n",
    "        logging.info(f\"Attempting to find content list by tag: {tag}\")\n",
    "\n",
    "    if not content_list:\n",
    "        logging.error(\"Couldn't find content list\")\n",
    "        return events\n",
    "\n",
    "    item_tag, item_attrs = next(iter(config['item_attr'].items()))\n",
    "    items = content_list.find_all(item_tag, **item_attrs)\n",
    "\n",
    "    for item in items:\n",
    "        event = {}\n",
    "        \n",
    "        title_tag, title_attrs = next(iter(config['title'].items()))\n",
    "        title_element = item.find(title_tag, **title_attrs)\n",
    "        if title_element:\n",
    "            event['title'] = title_element.text.strip()\n",
    "            url_element = title_element if title_element.name == 'a' else title_element.find_parent('a')\n",
    "            event['url'] = config[\"url\"] + url_element['href'] if url_element else ''\n",
    "        else:\n",
    "            logging.error(\"Couldn't find title element\")\n",
    "            continue\n",
    "\n",
    "        date_tag, date_attrs = next(iter(config['date'].items()))\n",
    "        date_element = item.find(date_tag, **date_attrs)\n",
    "        if date_element:\n",
    "            date_text = date_element.text.strip()\n",
    "            date_info = parse_date_range(date_text)\n",
    "            event.update(date_info)\n",
    "        else:\n",
    "            event.update({\n",
    "                'date': None,\n",
    "                'start_time': None,\n",
    "                'end_time': None\n",
    "            })\n",
    "\n",
    "        if config.get(\"img\"):\n",
    "            event['image_url'] = extract_image_url(config[\"img\"], item)\n",
    "        else:\n",
    "            event['image_url'] = None\n",
    "\n",
    "        location_tag, location_attrs = next(iter(config['location'].items()))\n",
    "        location_element = item.find(location_tag, **location_attrs)\n",
    "        event['location'] = location_element.text.strip() if location_element else None\n",
    "\n",
    "        recurrence_tag, recurrence_attrs = next(iter(config['recurrence'].items()))\n",
    "        recurrence_element = item.find(recurrence_tag, **recurrence_attrs)\n",
    "        event['recurrence'] = recurrence_element.text.strip() if recurrence_element else None\n",
    "\n",
    "        events.append(event)\n",
    "    \n",
    "    return events\n",
    "\n",
    "####################\n",
    "# EXECUTION\n",
    "####################\n",
    "\n",
    "def main():\n",
    "    for site_name, config in SITES.items():\n",
    "        url = config[\"url\"]\n",
    "        logging.info(f\"Fetching and parsing {site_name}\")\n",
    "\n",
    "        html_content = fetch_page(url)\n",
    "        if html_content:\n",
    "            save_html(html_content, site_name)  # Save HTML for debugging\n",
    "            grid_search(html_content)  # Perform grid search for debugging\n",
    "            \n",
    "            parsed_content = parse_html(html_content)\n",
    "            logging.info(f\"Length of parsed content: {len(str(parsed_content))}\")\n",
    "            find_potential_containers(parsed_content)  # Find potential containers for debugging\n",
    "            \n",
    "            events = extract_events(parsed_content, config)\n",
    "            logging.info(f\"Extracted {len(events)} events\")\n",
    "            for event in events:\n",
    "                logging.info(f\"Title: {event.get('title')}\")\n",
    "                logging.info(f\"URL: {event.get('url')}\")\n",
    "                logging.info(f\"Date: {event.get('date')}\")\n",
    "                logging.info(f\"Location: {event.get('location')}\")\n",
    "                logging.info(f\"Recurrence: {event.get('recurrence')}\")\n",
    "                logging.info(f\"Image URL: {event.get('image_url')}\")\n",
    "        else:\n",
    "            logging.error(f\"Failed to fetch or parse the content from {site_name}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ec602-dbd1-4325-a88a-cb9212a9a28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
